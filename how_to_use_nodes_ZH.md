# **节点使用说明**

## 节点概述
1. 本项目中的节点可以分为大模型节点、工具节点、函数节点、面具节点、加载器节点。
2. 工具节点是挂接在模型的tools接口上的节点，会在模型内部进行调用。对于工具节点，is_enable决定了该工具是否启用，方便用户快速变更模型上挂接的工具。
3. 面具节点和函数节点通常成对出现，面具节点为可以自定义的提示词模版，挂接在模型的system_prompt或者user_prompt接口。挂接面具节点的大模型有特定的输出格式，可以稳定调用对应的外部函数节点。
4. 加载器节点可以快捷的加载本地文件或者人格面具。

## api大模型节点
1. 大模型节点可以自定义模型名称、温度、API_KEY、base_url，目前暂时只支持openai类型的API接口调用。
2. 可以直接在节点上输入系统提示词、用户提示词，也可以右键将这两个小组件转化成节点的输入，接受字符串类型的输入。
3. 大模型节点还可以从tools接口接受工具节点的输出，可以从file_content接口接受字符串形式的输入，这些输入会被当作模型的知识库，以词向量相似度来搜索相关的内容输入到模型中。
4. 大模型节点的is_memory可以决定大模型是否拥有记忆，可以将is_memory改为disable，再运行，这时模型会清楚之前的对话记录，再切换回enable，之后的运行中模型就会保留与你的对话记录。
5. 可以通过assistant_response来查看本轮对话中模型的回复，也可以通过history来查看多轮对话的历史记录。
6. 即使外部参数不变，大模型节点总是会运行，因为大模型对同一个问题也总是有着不同的回答。
7. is_tools_in_sys_prompt决定了tools的信息是否会输入到系统提示词中。
8. is_locked可以锁住上轮对话的结果，让大模型直接返回上轮对话中的回答。

## 本地大模型
1. model_type目前可以选择GLM和llama格式。
2. model_path和tokenizer_path分别填入本地模型文件夹和本地分词器文件夹即可
3. is_reload决定了该节点运行后是否会卸载本地模型。开启后，每次都会重新加载模型，保证显存不被占用。关闭后，不会反复加载模型，缩短推理时间。
4. device决定了运行在cuda还是cpu，以及是否采用float16/int8/int4量化。
5. 其他参数均与api大模型节点一致.

## start_dialog节点和end_dialog节点
1. 这两个节点都有dialog_id，将dialog_id连接起来，让它们成一个对话存档点。当你需要将两个大模型进行循环连接时，虽然在comfyui中是不能实现的，但是可以将后一个模型的输出保存到本地，再下一次运行时，传递给前一个模型，可以使用comfyui API在其他前端中调用comfyui，只要循环调用，就可以看到两个模型的无限自我对话了。
2. start_dialog节点上有start_dialog的接口，可以作为一个对话开始时用户给出的提示词，引导大模型在用户给出的主题中讨论。

## google_tool节点
1. 可以输入你的google_api_key和cse_id来使用该节点
2. 该节点会返回谷歌搜索中的前10个网址和摘要部分，你可以要求模型翻页，来看更后面的搜索结果

## check_web_tool节点
1. 可以输入你想要搜索的网址到该节点中作为模型搜索的默认网址。
2. 由于request不是万能的，有些网址会不允许爬取，本项目也不提供恶意的爬虫代码

## time_tool节点和weather_tool节点
1. 用于查询时间和天气，time_tool节点可以更改查询的默认时区，weather_tool节点未来也会增加改变默认地区的选项
2. 未来会有更多的像这样的实用节点加入到本项目中

## interpreter节点
1. 可以让大模型生成Python代码后自动运行，并获得代码的运行结果
2. 暂时只支持Python代码

## load_file from comfyui_LLM_party/file节点
1. 读取文件的路径在comfyui_LLM_party/file，可以将你要读取的文件放到这个路径下，然后把文件名填入该节点即可
2. 输出是一个字符串，包含了文件中所有的文字信息

## file_conbine节点和tool_conbine节点
1. 用于将多个文件节点或者多个工具节点结合成一个，再输入到大模型中
2. 这些combine节点可以套娃使用，但是tool_conbine和file_conbine不能混用，tool节点的输出都是一个特定格式的json。
